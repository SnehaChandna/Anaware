import torch
import torch.nn as nn
import torch.nn.functional as F
from ultralytics import YOLO

class SectionTransformer(nn.Module):
    def __init__(self, input_dim, embed_dim, num_heads, num_layers, output_dim):
        """
        Args:
            input_dim (int): Number of input features per section.
            embed_dim (int): Dimension of the embedding space.
            num_heads (int): Number of attention heads.
            num_layers (int): Number of transformer encoder layers.
            output_dim (int): Dimension of the output feature vector.
        """
        super(SectionTransformer, self).__init__()
        
        # Initial BatchNorm layer
        self.batchnorm_initial = nn.BatchNorm1d(input_dim)
        
        # Embedding layer to project input features to embedding space
        self.embedding = nn.Linear(input_dim, embed_dim)
        
        # Positional encoding (optional, but recommended for sequences)
        self.positional_encoding = nn.Parameter(torch.zeros(1, 1000, embed_dim))
        
        # Transformer encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim,
            nhead=num_heads,
            dim_feedforward=2048,
            dropout=0.1,
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # BatchNorm layer applied after pooling the sequence
        self.batchnorm_after_pool = nn.BatchNorm1d(embed_dim)
        
        # Output layer to produce fixed-size feature vector
        self.output_layer = nn.Linear(embed_dim, output_dim)
    
    def forward(self, x):
        """
        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, input_dim).
        Returns:
            torch.Tensor: Output feature vector of shape (batch_size, output_dim).
        """
        # batch_size, seq_len, input_dim = x.shape
        
        # Permute to match BatchNorm1d expected shape: (batch_size, input_dim, seq_len)
        x = x.permute(0, 2, 1)  # (batch_size, input_dim, seq_len)

        # Normalize
        x = self.batchnorm_initial(x)  # (batch_size, input_dim, seq_len)

        # Permute back to (batch_size, seq_len, input_dim) for embedding layer
        x = x.permute(0, 2, 1)  # (batch_size, seq_len, input_dim)
        
        # Embed the input features
        x = self.embedding(x)  # (batch_size, seq_len, embed_dim)
        
        # Optionally add positional encoding:
        # x = x + self.positional_encoding[:, :seq_len, :]
        
        # Pass through the transformer encoder:
        x = self.transformer_encoder(x)  #(batch_size, seq_len, embed_dim)
    
        # Pool the sequence into a fixed-size vector (mean pooling)
        x = x.mean(dim=1)  # (batch_size, embed_dim)
        
        # Apply BatchNorm to stabilize feature distribution
        x = self.batchnorm_after_pool(x)  # (batch_size, embed_dim)
        
        # Final output layer
        x = self.output_layer(x)  # (batch_size, output_dim)
        
        return x

class ThreeLayerNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        """
        A three-layer neural network with batch normalization.
        
        Args:
            input_size (int): Dimensionality of the input features.
            hidden_size (int): Number of neurons in the hidden layers.
            output_size (int): Dimensionality of the output.
        """
        super(ThreeLayerNN, self).__init__()
        # Adding 1 to input_size for the probability input.
        self.fc1 = nn.Linear(input_size + 1, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, output_size)
        
        self.bn1 = nn.BatchNorm1d(hidden_size)
        self.bn2 = nn.BatchNorm1d(hidden_size)
        
        self.gelu = nn.GELU()
        self.prob = nn.Sigmoid()
        
    def forward(self, x, prob):
        """
        Args:
            x (torch.Tensor): Input feature tensor.
            prob (torch.Tensor): Additional probability input.
        Returns:
            torch.Tensor: Output probability.
        """
        # Concatenate the probability with the input features
        x = torch.cat((x, prob.unsqueeze(1)), dim=1)
        
        x = self.fc1(x)
        x = self.bn1(x)
        x = self.gelu(x)
        
        x = self.fc2(x)
        x = self.bn2(x)
        x = self.gelu(x)
        
        x = self.fc3(x)
        x = self.prob(x)  # Output probability
        
        return x

class MalwareDetector(nn.Module):
    def __init__(self, input_dim, embed_dim, num_heads, num_layers, output_dim, yolo_model, hidden_NN, output_NN):
        super(MalwareDetector, self).__init__()
        self.featureTransformer = SectionTransformer(input_dim, embed_dim, num_heads, num_layers, output_dim)
        self.yolo = YOLO(yolo_model)
        self.merger = ThreeLayerNN(output_dim, hidden_NN, output_NN)
        
    def forward(self, sections, imgs):
        batch_size = len(sections)
        features = self.featureTransformer(sections)
        imgs = imgs.unsqueeze(0)
        imgs = torch.tensor(imgs, dtype=torch.float32)  # shape: [256, 256]
        imgs = imgs.repeat(1, 3, 1, 1)  # now shape: [1, 3, 256, 256]
        img_probs = torch.stack([result.probs.data[1] for result in self.yolo.predict(imgs, batch=batch_size, verbose=True)])
        return self.merger(features, img_probs)
    
    def train(self):
        self.featureTransformer.train()
        self.merger.train()
    
    def eval(self):
        self.featureTransformer.eval()
        self.merger.eval()